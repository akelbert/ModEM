\section{Mathematical Background}

\subsection{Linearized EM inversion}

We consider regularized inversion based on 
gradient-based minimization of a penalty functional of the form
\begin{equation}
\label{e.PEN}
\cJ (\mm , \dd ) =  (\res )^T \CdI (\res ) + 
\nu (\mm - \mm_0 )^T \CmI (\mm - \mm_0 ) 
\end{equation}
to recover, in a stable manner, 
$\mm$, an $M$-dimensional Earth conductivity model parameter vector,
which provides an adequate fit to a data vector
$\dd$ of dimension $N_d$. In (\ref{e.PEN}) $\Cd$ is the covariance
of data errors, $\fm$ defines the forward mapping (which must
be computed numerically),
$\mm_0$ is a prior or first guess model parameter, $\nu$ is
a regularization parameter, and $\Cm$ (or more properly
$\nu^{-1} \Cm$) defines the model covariance or regularization term.  
In practice $\Cd$ is always taken to be diagonal, so by a simple
rescaling of the data and forward mapping 
(${\bf C_d}^{-1/2} \dd$, ${\bf C_d}^{-1/2} \FF$),
we may eliminate $\CdI$ from the definition of $\cJ$.

The prior model and model covariance $\Cm$
can also be eliminated from (\ref{e.PEN})
by the affine linear transformation of the model parameter
$\tilde \mm = \Cm^{-1/2}(\mm - \mm_0)$,
and forward mapping 
$\tilde{\bf f} (\tilde{\bf m} ) = {\bf f} (\Cm^{1/2}\tilde{\bf m} )$,
reducing (\ref{e.PEN}) to
\begin{equation}
\label{e.PEN_tilde}
\cJ (\tilde \mm , \dd ) =  (\resT )^T (\resT ) + 
\nu \mm^T\mm  = || \resT ||^2 = \nu || \mm ||^2   .
\end{equation}
After minimizing (\ref{e.PEN_tilde}) over $\tilde \mm$ the 
model parameter in the untransformed space can be
easily recovered as $\mm = \Cm^{1/2}\tilde \mm$.  
Note that this model space transformation is in fact quite practical if
instead of following the usual practice of defining
$\CmI = \DD^T\DD$, where $\DD$ is a discrete representation of a gradient 
or higher order derivative operator,
the regularization is formulated directly in terms of a smoothing
operator (i.e., model covariance) $\Cm$.
It is relatively easy to construct computationally efficient positive
definite discrete
symmetric smoothing operators for regularization
(e.g., Egbert et al., 1994; Siripunvaraporn and Egbert, 2000; 
Chua and Bennett, 2001).
Although the resulting covariance
matrix $\Cm$ will not generally be sparse or practical
to invert, all of the computations required for
gradient computations and for minimization of
the transformed penalty functions require only multiplication by the smoothing
operator $\Cm^{1/2}$ (i.e, half of the smoothing of $\Cm$).
It is also  quite possible to define the covariance so that
multiplication by either $\Cm$ or $\CmI$ is practical.

In the following we consider only the simplified ``canonical'' penalty 
functional (\ref{e.PEN_tilde}),
with tildes omitted.  We first summarize standard approaches
to minimization using a consistent notation.
Siripunvaraporn and Egbert (2000), Rodi and Mackie (2001), 
Newman and Boggs (2004), and Avdeev (2005) 
provide further details and discussion on these and related methods.
Before proceeding we note that
many of the computations in frequency domain 
EM problems are most efficiently implemented
(and described) using complex arithmetic, but the model
conductivity parameter $\mm$ is real.  
Data might be complex (an impedance)
or real (an apparent resistivity and/or phase).  
For simplicity we will always assume that all data are real, i.e.,
real and imaginary parts of a complex impedance element 
will be treated as separate elements of 
the real data vector $\dd$, and we assume that all operators 
have been recast to map real vectors to real vectors.
Note, however, that in practice some computations are implemented 
most efficiently using complex arithmetic, and this must
be taken into account to develop a practical and
efficient inversion system, as discussed briefly below (and perhaps
in the Appendix?).

The derivative of $\FF$ with respect to
the model parameters is the $N_d \times M$ 
Jacobian (or sensitivity matrix) $\JJ$, with $ij$ element 
$J_{ij} = \partial f_i / \partial m_j$.  
As we show below,
for general EM problems the Jacobian can be formally written
\begin{equation}
\label{e.SensModel}
\JJ = \LL {\bf S}_{\mm}^{-1} \PP + \QQ  ,
\end{equation}
where $\LL$ represents the measurement process (i.e., the
linearized data functionals);
${\bf S}_{\mm}^{-1}$ denotes the EM solution operator
for conductivity distribution $\mm$;
and $\PP$ and $\QQ$ are, respectively, $N_e \times M$
and $N_d \times M$ matrices related to the 
model parameterization (of dimension $M$), numerical
EM solution representation (of dimension $N_e$), 
and the $N_d$ measurement functionals.
Newman and Alumbaugh (1997), Spitzer (1998)
and Rodi and Mackie (2001) provide more detailed discussions
of (\ref{e.SensModel}) for some specific EM inverse problems,
and we will consider the general case extensively below.
The key points for our immediate discussion
are (1) computation of the sensitivity matrix requires
application of the forward solver $M$ times (once for
each column of $\JJ$), or the transpose (adjoint) of this solver
(${{\bf S}_{\mm}^T}^{-1}$)  $N_d$ times (once for each row of $\JJ$);
and (2) multiplication of an arbitrary model (data) space vector by 
$\JJ$  (by $\JJ^T$) requires one forward (one adjoint) solution.
Because the EM problem is essentially self-adjoint (i.e.,
in physical terms, reciprocity holds) solving the adjoint
problem is essentially equivalent to solving the forward problem,
and we often ignore this distinction when discussion
computational burdens.

Gradient-based search for a minimizer of
(\ref{e.PEN_tilde}) using $\JJ$ is iterative, as for example
in the classical Gauss-Newton (GN) method.  
Let $\mn$ be the model parameter
at the $n$th iteration, 
$\Jn$ the sensitivity matrix evaluated at $\mn$, 
and $\resn = \dd - \Fmn$ the data residual.  Then
linearizing the penalty functional in the vicinity of $\mn$ for
small perturbations $\delta \mm$ leads to the $M \times M$ system
of normal equations
\begin{equation}
\label{e.modSpace}
(\JnT \Jn + \nu \II)\delta \mm = \JnT \resn - \nu \mn ,
\end{equation}
which can be solved for $\delta \mm$
to yield a new trial solution  $\mm_{n+1} = \mn + \delta \mm$.
As discussed in Parker (1994) this basic linearized scheme generally
requires some form of step length damping for stability
(e.g., a Levenberg-Marquardt approach; Marquardt, 1963;
Rodi and Mackie, 2001).

There are two variants on the standard GN approach relevant
to our development.
In the Occam approach (Constable et al., 1987; Parker, 1994)
(\ref{e.modSpace}) is rewritten
\begin{equation}
\label{e.modSpace0}
(\JnT \Jn + \nu\II )\mm  = \JnT \Dh  ,
\end{equation}
where $\Dh = \dd - \Fmn + \Jn \mn$.
Although $\mm_{n+1}$ is obtained directly
as the solution to (\ref{e.modSpace0})
the result is exactly equivalent to solving (\ref{e.modSpace})
for the change in the model at step $n+1$ and adding the result to $\mm_n$.
A more substantive difference is that
in the Occam scheme step length control is achieved
by varying $\nu$, computing a series of
trial solutions $\mm_{\nu}$ to (\ref{e.modSpace0}), and solving the 
forward problem for each $\mm_{\nu}$
to evaluate the actual data misfit.  
%In phase I of the
%Occam process $\nu$ is chosen to minimize data misfit; as the scheme converges
%$\nu$ is chosen to minimize the model norm while keeping the misfit constant
%(phase II).  
An advantage of this approach is that 
the regularization parameter $\nu$ is determined
as part of the search process, and at convergence one is assured
that the solution attains at least a local minimum of the
model norm $ || \mm || = (\mm^T \mm )^{1/2}$, subject to the data
fit attained (Parker, 1994).
In other approaches $\nu$
must be varied independently to choose an optimal value
(e.g., Newman and Alumbaugh 2000; Newman and Boggs, 2004).

The Occam scheme can also be implemented in the data space 
(Siripunvaraporn and Egbert, 2000;
Siripunvaraporn et al., 2005).  The solution to
(\ref{e.modSpace0}) can be written as
\begin{equation}
\label{e.DataSpace0}
\mm_{n+1} = \Cm \JnT \xX
\end{equation}
where the coefficients $\xX$ satisfy 
\begin{equation}
\label{e.DataSpaceEq}
( \Jn \JnT + \nu^{-1} \INd ) \xX =  \Dh ,
\end{equation}
as can be verified by substituting
(\ref{e.DataSpace0}) and (\ref{e.DataSpaceEq}) into (\ref{e.modSpace0})
and simplifying.
This ``data space Occam'' (DASOCC) approach requires
solving an $N_d \times N_d$ system of equations in the data space
instead of the $M \times M$ model space system of equations (\ref{e.modSpace0}).
If the model is heavily over-parameterized 
$N_d$ may be much smaller than $M$, making the
DASOCC scheme significantly more efficient than equivalent model 
space approaches.  

Computing the full Jacobian $\JJ$ required
for the three variants on the GN approach described above
is a very demanding computational task
for multi-dimensional EM problems, since the equivalent
of one forward solution is required for each row (or column) of $\JJ$.
An alternative approach is to solve these equations with
an iterative solver such as conjugate gradients (CG).  This
requires computation of matrix vector products
such as $\Aa \mm = [ \JJ^T \JJ + \nu \II] \mm$, which
(as (\ref{e.SensModel}) already shows)
can be accomplished without forming
or storing $\JJ$ at the cost of two forward solutions
(e.g., Mackie and Madden; 1993).
Mackie and Madden (1993),
Zhang et al. (1995), Newman and Alumbaugh (1997),
Rodi and Mackie (2001), and others, have used
CG to solve (\ref{e.modSpace}),
while Siripunvaraporn and Egbert (2007) have applied the same approach
to the corresponding data space equations of (\ref{e.DataSpace0}).

Whether a model or data space approach is used,
CG iteratively solves for $\mm_{n+1}$ 
based on linearization in the vicinity of $\mm_n$.
This iterative solver is then embedded in a
further outer loop (over $n$) 
to solve the non-linear inverse problem. 
Alternatively one can apply
CG directly as a non-linear quadratic optimization scheme 
(e.g., Press et al., 1986)
to minimize (\ref{e.PEN}) (e.g., Rodi and Mackie, 2001; 
Newman and Boggs, 2004; Avdeev, 2005; Kelbert, 2008).  With this
NLCG approach one must evaluate the gradient of (\ref{e.PEN})
with respect to variations in model parameters $\mm$ :
\begin{equation}
\label{e.Grad}
\left. \frac{\partial \cJ} {\partial \mm} \right |_{{\bf m}_n}  = 
- \JnT \resn  + \nu \mn   .
\end{equation}
The gradient is then used to calculate a new ``conjugate''
search direction in the model space.  After minimizing
the penalty functional along this direction,
using a line search which requires at most a few evaluations
of the forward operator, the gradient is recomputed.
NLCG again utilizes essentially the same
basic computational steps as required for solving
the linearized equations (\ref{e.modSpace}) (i.e.,
the forward problem must be solved to evaluate $\FF(\mm)$,
and for the line search,
and (\ref{e.SensModel}) implies that
multiplication of the residual $\resn$ by $\JnT$
(required to compute the gradient),
can be accomplished by solving the adjoint problem
(e.g., Newman and Alumbaugh, 2000).
Quasi-Newton schemes (QN; e.g., Nocedal and Wright, 1999; Newman and Boggs, 2004;
Haber, 2005; Avdeev, 2005) provide an alternative
approach to NLCG for direct minimization of (\ref{e.PEN}),
with similar advantages with regard to storage and computation
of the Jacobian, and similar computational steps.

The key point here is that
all of these gradient based schemes for minimizing
(\ref{e.PEN}) can be abstractly expressed in terms of
a small number of basic objects 
(data and model parameter vectors, $\dd$ and $\mm$), and
operators (the forward mapping $\FF(\mm)$, 
multiplication by the corresponding Jacobian $\JJ$
its transpose $\JJ^T$,
and the data and model covariances $\Cm$ and $\Cd$).

So far we have ignored an
important issue that will be fairly central
to the structure of the modular system:
in most cases EM data are obtained for a wide
range of frequencies (MT),
and/or for a number of different transmitter
configurations (controlled source problems).
Each of these different frequencies or source configurations
(generically referred to as ``transmitters'')
requires solving a different forward problem.
In this common situation the data vector and
forward modeling operator can be decomposed into 
$N_T$ blocks, one for each transmitter
\begin{equation}
\label{e.dPartition}
\dd = \left(
\begin{array}{c c c}
\dd_1^T & ... & \dd_{N_T}^T
\end{array}
\right)
\,\,\,\,\,\,\,\,\,
\FF = \left(
\begin{array}{c c c}
\FF_1^T & ...& \FF_{N_T}^T 
\end{array}
\right)  .
\end{equation}
The Jacobian, and
the matrices in the general expression for $\JJ$ in (\ref{e.SensModel}),
can be similarly partitioned into $N_T$ blocks each in the obvious way.
We return to the issue of multiple transmitters below,
but for our initial discussion of Jacobian calculations 
we consider only the simpler case of a single transmitter.

\subsection{Data Sensitivities}

We first justify
the general expression for the sensitivity matrix
for a single transmitter (\ref{e.SensModel}),
omitting reference to the iteration index $n$.
The numerical discretization of the frequency domain EM 
differential equation is written generically as
\begin{equation}
\label{e.genericEM}
{\bf S}_\mm \ee = \bb
\end{equation}
where $\bb$ gives appropriate boundary and forcing terms
for the particular EM problem,
$\ee$ is the $N_e$ dimensional vector representing
the discretized electric and/or magnetic fields
(or perhaps potential functions), and
${\bf S}_{\mm}$ is an $N_{\bf e} \times N_{\bf e}$
matrix which depends on the
$M$ dimensional model parameter $\mm$.
Given a solution $\ee$ to (\ref{e.genericEM})
data can always be written in the general form
\begin{equation}
\label{e.genericData}
d_j = f_j( \mm ) + \epsilon_j = \psi_j(\ee(\mm),\mm) + \epsilon_j
\end{equation}
where $\psi_j$ is some generally non-linear, but usually simple,
function of the components of $\ee$ (and possibly $\mm$), and
$\epsilon_j$ represents data error.

With this general setup we have, by the chain rule
\begin{equation}
\label{e.genericDeriv}
J_{jk} = \frac{\partial f_j}{\partial m_k} =
\sum_l \frac {\partial \psi_j}{\partial e_l}
\frac{\partial e_l}{\partial m_k} +
\frac{\partial \psi_j}{\partial m_k} ~~~~ .
\end{equation}
Letting ${\Fmat , \LL , \QQ}$ be the partial derivative matrices
\begin{equation}
\label{e.LL_QQ}
F_{lk} = \left. \frac{\partial e_l}{\partial m_k} 
\right |_{\mm_0}
\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
L_{jl} = \left. \frac{\partial \psi_j}{\partial e_l} 
\right |_{{\bf e}_0,\mm_0}
\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
Q_{jk} = \left. \frac{\partial \psi_j}{\partial m_k} 
\right |_{{\bf e}_0,\mm_0},
\end{equation}
where $\ee_0$ is the solution to (\ref{e.genericEM})
for model parameter $\mm_0$,
the Jacobian at $\mm_0$ can be written in matrix notation as
\begin{equation}
\label{e.matrixDeriv}
\JJ = \LL \Fmat  + \QQ  .
\end{equation}
The rows of $\LL$
(one for each observation) are generally very sparse, supported only on
a few nodes surrounding the corresponding data site.
When the observation functionals
are independent of the model parameters
(as they often are) $\QQ \equiv \bf 0$.
When $\QQ$ is required it is also typically sparse,
but this depends on the specific nature of the model parameterization.
The $j$th row of $\LL$ represents
the linearized data functional, which is applied to the perturbation
in the EM solution to compute the perturbation in $d_j$.
Although, as we show below,
derivation of expressions for $\LL$ and $\QQ$ can be quite involved for
realistic EM data functionals,
calculation of ${\bf F}$ presents the only
real computational burden.

To derive a general expression for $\Fmat$ take the derivative
of (\ref{e.genericEM}) with respect to the model parameters $\mm$.
Assuming $\bb$ remains fixed (if this is not true minor modifications
are necessary; Newman and Boggs (2004) treat this additional
complication), and allowing that as $\mm$ is varied
the solution $\ee$ also varies, we obtain
\begin{equation}
\label{e.genericEMderiv}
\left. {\bf S}_{\mm_0} \left [ \frac{\partial \ee}{\partial \mm}
\right |_{\mm = {\bf m}_0} \right]
= - \frac{\partial {\bf S}_{\mm}
\ee_0}{\partial \mm},
\end{equation}
or
\begin{equation}
\label{e.genericEMderiv1}
{\bf S}_{\mm_0} \Fmat = \PP .
\end{equation}
The $N_e \times M$ matrix $\PP$ depends on details
of both the numerical model implementation and the conductivity
parameterization, but is in general inexpensive to calculate.
Putting together (\ref{e.matrixDeriv}) and (\ref{e.genericEMderiv1})
we obtain expression (\ref{e.SensModel}).

Computing all of $\JJ$ would appear to require
solving the induction equation once for each of the
$M$ columns of $\PP$.
However, simply taking the transpose of (\ref{e.SensModel}) we obtain
\begin{equation}
\label{e.SensAdjtApp}
\JJ^T = \PP^T [{\bf S}_{\mm}^T]^{-1} \LL^T + \QQ^T  ,
\end{equation}
so the sensitivity matrix can in fact be obtained by solving
the transposed discrete EM system $N_d$ times
(once for each column of $\LL^T$),
the usual "reciprocity" trick for efficient
calculation of sensitivities (e.g., Rodi, 1976, de Lugao {\it et al.}, 1997).

Note that the EM equations are self-adjoint 
(except for time reversal) with respect to
the usual $L^2$ inner product (i.e., reciprocity
holds), so on a uniform grid $\Ss$ is symmetric
(leaving aside issues regarding boundary conditions).  
For more general grids the fact that the EM operator is self-adjoint
implies 
\begin{equation}
\label{e.Sadj}
\Ss^T = {\bf V} \Ss {\bf V}^{-1},
\end{equation}
where ${\bf V}$ is a diagonal matrix of
integration volume elements for the natural 
discrete representation of the $L_2$
integral inner product on the model domain.   
Eq. (\ref{e.Sadj}) implies
$\Ss^T {\bf V} = {\bf V} \Ss $ is a symmetric
(though not Hermitian) matrix.
It is easier to compute solutions to this symmetrized problem,
so solutions to the forward problem are generally computed
as $\ee =  ({\bf V} \Ss ) ^{-1} {\bf V} \bb$.
The solution for the adjoint problem can also be written
in terms of the symmetrized inverse operator as
$\ee = (\Ss^T)^{-1} \bb = {\bf V} ({\bf V} \Ss ) ^{-1} \bb$; the
only difference from the forward case is thus the
order in which multiplication by the diagonal matrix $\bf V$ and
the symmetrized solver are called.

\subsection{Examples: 2D and 3D Magnetotellurics}

To derive more explicit expressions for the operators
$\LL$, $\PP$, and $\QQ$, and hence $\JJ$,
more specific assumptions about the numerical implementation
of the forward problem (\ref{e.genericEM}) are required.  To motivate
the general development we consider two specific cases:
inversion of 2D and 3D MT data, 
using a finite difference modeling approach,
but most of the results obtained are more broadly applicable.
Initially we consider only a single fixed frequency,
and begin with the 3D case.

In the quasi-static limit appropriate for MT,
Maxwell's equations in the frequency domain (i.e., assuming
a time dependence of $e^{i\omega t}$) can be
expressed as a second order elliptic system of
partial differential equations in terms of the electric fields alone,
which in continuous form is
\begin{equation}
\label{e.IndE}
\nabla \times \nabla \times  {\bf E} + i \omega \mu \sigma {\bf E} = 0
\,\,\,\,\,\,\,\,\, ( + \,\,\,\, {\rm BC}) .
\end{equation}
To solve (\ref{e.IndE}) numerically in 3D, we consider a
finite difference approximation on a staggered grid
of dimension $N_x \times N_y \times N_z$, as
illustrated in Figure 1
(e.g., Yee, 1966; Smith, 1996; Siripunvaraporn et al., 2002).
In the staggered grid formulation
the discretized electric field vector components
are defined on cell edges (Figure 1).
We denote the space of such finite dimensional
``cell edge'' vector fields
by $\bar{\cal S}_P$, where the subscript P
is used to indicate that this space contains the ``primary''
EM field, that which is solved for.  A typical element will be
denoted by $\ee$.
As illustrated in Figure 1, the magnetic fields, which
in continuous form
satisfy $ \BB = (-i \omega)^{-1} \nabla \times {\bf E}$,
are naturally defined on the discrete grid on
cell faces.  We denote the
space of discrete vector fields defined on faces 
by $\bar {\cal S}_A$, where now the subscript A is
used to indicate that the space contains the
``auxiliary'' EM field, i.e., the field which is
not directly solved for.  A typical element of the auxiliary field
space will be denoted by $\tilde \ee$.  
In fact, only edges in the interior of the grid 
are solved for; the tangential components on
the boundary edges are provided as input data.
We denote the space of these interior nodes
(of dimension $N_e = 3N_x N_y N_z-N_x N_y - N_x N_z - N_y N_z$)
by ${\cal S}_P$, and the corresponding space of interior faces
by ${\cal S}_A$, i.e., with overbars omitted.

In our 3D example the primary and auxiliary
field components represent electric and magnetic fields, respectively.
It is also possible to form the quasi-static magnetic induction equation 
\begin{equation}
\label{e.IndB}
\nabla \times \rho \nabla \times  {\bf B} - i \omega \mu {\bf B} = 0
\,\,\,\,\,\,\,\,\, ( + \,\,\,\, {\rm BC}) ,
\end{equation}
and solve directly for $\BB$, which then would 
be the primary field, in our terminology.
In this case the electric field is computed as
${\bf E} = \rho\nabla\times\BB$, and would be the auxiliary field.
For the case (\ref{e.elecEM}) the discrete auxiliary field
is related to the primary via 
\begin{equation}
\tilde \ee = (-i\omega)^{-1} \CC \ee ,
\end{equation}
where $\CC:\bar{\cal S}_P \mapsto {\cal S}_A$ is the discrete approximation
of the curl of interior cell edge vectors.  The discrete
operator in (\ref{e.genericEM}) can thus be expressed as
\begin{equation}
\label{e.elecEM}
{\bf S}_{\mm} = \CC^{\dag}\CC + {\bf diag} [i \omega \mu \sigma (\mm)] .
\end{equation}
Here $\CC^\dag:{\cal S}_A \mapsto {\cal S}_P$ is the discrete curl
mapping interior cell face vectors to interior cell edges.
As the notation indicates this operator is the
adjoint of $\CC$, relative to appropriate (i.e., volume
weighted) inner products on the
spaces ${\cal S}_A$ and ${\cal S}_P$.  For completeness,
further discussion of these operators are given in the Appendix.
In particular we provide details on treatment of interior and boundary nodes,
and on how the right hand side  $\bb$ of (\ref{e.genericEM})
is determined from the boundary data.

In (\ref{e.elecEM}) the dependence
of the operator coefficients on the model parameter
(which we take to be an element of some finite dimensional
space $\cal M$)
is made explicit through the mapping
$\sigma : {\cal M} \mapsto {\cal S}_P$,
where $\sigma(\mm)$ is a real vector (defined on
cell edges) which
represents the average conductivity of surrounding cells.
As a specific example we consider the simplest
model parameterization, with conductivity, or the natural
logarithm of conductivity, specified independently for each of the
$M = N_x N_y N_z$ cells in the numerical grid.
We denote by $\WW$ the $N_e \times M$ matrix which defines
the volume weighted average from the
four surrounding cells to each of the interior edges.
Then a physically consistent (current conserving)
mapping of conductivity to cell edges is given by
$\sigma (\mm) = \WW \mm $ or $\sigma(\mm)=\WW {\bf exp} (\mm)$,
for the cases of linear and log conductivity, respectively.

It is also instructive to consider the
2D MT inverse problem.  Now there are effectively two distinct
modeling problems: for TE and TM modes, with
electric and magnetic fields, respectively, parallel
to the geologic strike.
The TE mode case is essentially identical to the 3D case already
discussed.  
The TM mode case, which is solved in terms of the magnetic field
instead of the electric field, is more instructive
with regard to generalization.
In the TM mode the magnetic field parallels
the geological strike ($x$) and (\ref{e.IndB}) can
be reduced to a scalar partial differential 
equation (PDE) in the $y-z$ plane
\begin{equation}
\label{e.TMeq}
\partial_y \rho \partial_y B_x + \partial_z \rho
\partial_z B_x +  i \omega \mu B_x = 0 \,\,\,\, (+BC) .
\end{equation}

As for the 3D problems, for the discrete 2D problem we can define
finite dimensional spaces of primary (${\cal S}_P$)
and auxiliary (${\cal S}_A$) EM fields.  Now the the primary
field is $B_x$, defined on the nodes (corners) of the 2D grid,
and the auxiliary fields are the electric field components
$E_y$ and $E_z$ defined on the vertical and horizontal
cell edges (Figure 2).  A natural centered finite difference approximation
of (\ref{e.TMeq}) can be written in terms of a discrete
2D gradient operator ${\bf G}:{\cal S}_P \mapsto {\cal S}_A$
and a 2D divergence operator ${\bf D}:{\cal S}_A \mapsto{\cal S}_P$.
Using $\ee \in {\cal S}_P$ to denote the primary
discrete EM field solution ($B_x$) we have a more explicit
form for (\ref{e.genericEM}) for this discrete
TM mode implementation
\begin{equation}
\label{e.TMeqDisc}
\left [
\DD {\bf diag } \left[
\rho(\mm) \right]  \GG \ee -  i\omega\mu\II \right] \ee = \bb .
\end{equation}
Again, there are some minor technical issues with
regard to boundary conditions (which effectively define $\bb$),
which are discussed for completeness in the Appendix.

In (\ref{e.TMeqDisc}) the PDE coefficients depend on the
model parameters through $\rho : {\cal M} \mapsto {\cal S}_A$,
i.e., the resistivity $\rho(\mm)$ defined on the auxiliary
grid, cell edges.  To be specific,
we again consider the simplest model parameterizations,
with conductivity or log conductivity for each cell
in the numerical grid an independent parameter.
From physical considerations, it is most reasonable to
compute the required edge resistivities from cell conductivities
by first transforming to resistivity,
and then computing the area weighted average of resistivities
of the two cells on either side of the edge.  Representing the
averaging operator from 2D cells to cell sides as $\WW_{TM}$,
and letting $(\mm)^{-1}$ denote the
component-wise inverse of the model parameter vector,
we then have
\begin{equation}
\label{e.WTMlin}
\rho(\mm) = \WW_{TM} (\mm)^{-1}
\end{equation}
\begin{equation}
\rho(\mm) = \WW_{TM} {\bf exp}(-\mm)
\end{equation}
for linear and log conductivity respectively.

The specific examples discussed here 
illustrate key points which
will hold for many other EM modeling approaches.
In particular, there are two spaces of discrete vectors on 
conjugate grids ${\cal S}_P$ and ${\cal S}_A$,
defining primary and auxiliary fields, respectively.
The equations are reduced to a second order equation in
the primary fields, but for several reasons it is
worthwhile to explicitly consider the auxiliary fields defined
on the conjugate grid as well.  First, in all of the examples 
considered the auxiliary fields correspond to electric
or magnetic fields, which the data functionals discussed
below will depend on.
Second, the dependence of the discrete PDE
operator coefficients on the model parameter can
generally be represented explicitly through a mapping
$\pi(\mm)$ from the model parameter space $\cal M$ to
either ${\cal S}_P$ or ${\cal S}_A$.

We emphasize here that our formulation would apply
to any basis functions used to represent conductivity 
or log conductivity in the model domain--only
the matrices $\WW$ or $\WW_{TM}$ which define
the appropriate volume or area averages of conductivity
or resistivity would change.  Other mappings from 
a model parameter space (e.g., a sharp boundary inversion,
where model parameters define the position of a layer interface)
also fit within this general framework,
although the explicit form for the mapping $\pi(\mm)$ would
become more complicated.

In other finite difference modeling approaches, 
e.g. if Maxwell's equations
are cast in terms of vector potentials, similar (although
somewhat more complicated) sets of conjugate spaces can
be defined, the differential operator can be decomposed into
discrete approximations to first order linear differential
operators which map between conjugate grids, and the 
dependence of discrete operator coefficients on an abstract
model parameter space can be described through a mapping
$\pi : {\cal M} \mapsto {\cal S}_{P,A}$.  Finite element approaches
to EM modeling will result in similar structures, with 
conjugate grids now representing spaces of element nodes
(or edges) and element interior quadrature points.

\subsection{Implementation of $\PP$, $\LL$ and $\QQ$}

To derive a general form for the matrix $\PP$
we write the discrete EM operator as
\begin{equation}
\label{e.SSgeneral}
{\bf S}_{\mm} = {\bf S}_0 + \tilde \RR {\bf diag}[ \pi (\mm) ] \RR.
\end{equation}
The 3D finite difference staggered grid electric field  
(${\bf S}_0 = \CC^{\dag}\CC$, $\tilde\RR = i\omega\mu\II$,
$\RR = \II$, $\pi(m) = \sigma(m)$), and the 2D TM mode case
(${\bf S}_0 = i\omega\mu\II$, $\tilde\RR = \GG$, $\RR = \DD$,
$\pi(\mm) = \rho(\mm)$) are special cases.
Then, writing out the components of
$\PP$ (see (\ref{e.genericEMderiv}))
\begin{eqnarray}
\label{e.3DE_P}
P_{ji} = \frac{\partial [ {\bf S}_{\mm} \ee_0]_j}{\partial m_i} = 
\frac{\partial \left [ \sum_k \tilde{R}_{jk} \pi_k (\mm)
\sum_l R_{kl} e_{0l}\right]}{\partial{m_i}} \\
= \sum_k \tilde{R}_{jk}  \left [ \sum_l R_{kl} e_{0l}\right] \nonumber
\frac{\partial\pi_k (\mm)}{\partial{m_i}}
= \left[ \tilde \RR {\bf diag} [ \RR \ee_0 ]
\frac{\partial \pi}{\partial \mm} \right]_{ji} .
\end{eqnarray}
Thus we have the explicit matrix expression for $\PP$ for the
general case (\ref{e.SSgeneral})
\begin{equation}
\label{e.PP}
\PP = \left [ \tilde \RR {\bf diag} [ \RR \ee_0 ] \right ]
\frac{\partial \pi}{\partial \mm} .
\end{equation}

For the 3D MT case with the model parameterized
using log conductivity for each cell in the numerical
discretization we find
\begin{equation}
\label{e.PPlog}
\PP = \left[ {\bf diag}(i \omega \mu \ee_0) \right ]
\left [ \WW {\bf diag}[\exp(\mm_0)] \right] .
\end{equation}
For the 2D TM case with the comparable simple logarithmic
conductivity parameterization we have
\begin{equation}
\label{e.PPTMlog}
\PP = \left [ \DD {\bf diag } 
\left [ \GG \ee_0 \right ] \right]
\left [ \WW_{TM} {\bf diag}[\bf{exp}(-\mm_0)] \right ] .
\end{equation}
Slightly simpler expressions are obtained in the case
of a linear conductivity parameterization.

Note that in all cases multiplication by $\PP$
involves application of simple diagonal, local
averaging, and finite difference operators.
Multiplication by the transpose $\PP^T$ requires
adjoints of the finite difference and averaging
operators, but otherwise presents no significant complications.
For example, for (\ref{e.PPTMlog}) the adjoint is
given by
\begin{equation}
\label{e.PPTMlogAdj}
\PP^T = 
\left [ {\bf diag} [\bf{exp}(-\mm_0)]
\WW_{TM}^T \right] \left[
{\bf diag } \left [ \GG \ee_0 \right ] 
\DD^T \right] . 
\end{equation}
Note the adjoint of the averaging operators
considered here ($\WW$ and $\WW_{TM})$
represent mappings from cell edges
to cells, summing contributions from all edges that
bound a cell. 

Eq. (\ref{e.PP}) provides a broadly applicable recipe
for construction of the operators $\PP$ and $\PP^T$.
A key point to emphasize is that these operators divide
into two components, indicated by the brackets in
(\ref{e.PPlog})-(\ref{e.PPTMlogAdj}).  Only the second part
depends on details of the model parameterization.

The matrices $\LL$ and $\QQ$ represent linearizations
of the data functionals.  $\LL$ connects perturbations
in the primary EM solution ($\delta \ee$) to perturbations
in the data; $\QQ$ gives the component of variations in
the observations due to possible dependence of the
data functionals on the model parameters.
In general, EM data used in an inversion are
functions of electric and magnetic field components
observed at one or more locations. These data functionals can thus
always be expressed as
\begin{equation}
\psi_j(\ee(\mm),\mm) = 
\zeta_j(\vec\theta(\ee,\mm)) =
\zeta_j(\theta_1(\ee,\mm), ... , \theta_K(\ee,\mm)) =
 \zeta_j(\lambda_1(\mm)^T \ee, ... , \lambda_K(\mm)^T \ee ),
\end{equation}
where $\lambda_k(\mm)  , k = 1, ..., K$
represent more basic functionals which evaluate an electric or
magnetic component at a point in the model domain.
From the definition in (\ref{e.LL_QQ})
the $jth$ row of $\LL$ and of $\QQ$ are then given by
\begin{equation}
\label{e.Lj}
{\bf l}_{j} = \frac{\partial \psi_j}{\partial \ee} = 
\left . \sum_k \frac{\partial \zeta_j}{\partial \theta_k}
\right|_{\vartheta(\ee_0,\mm_0)} \lambda_k(\mm)^T = 
\sum_k \alpha_{jk} \lambda_k(\mm)^T,
\end{equation}
\begin{equation}
{\bf q}_{j} = \frac{\partial \psi_j}{\partial \mm} = 
\sum_k \alpha_k \frac{\partial \lambda_k(\mm)^T \ee_0}
{\partial \mm} .
\end{equation}

The field component
evaluation functionals $\lambda_k$ are represented by
vectors in the primary solution space ${\cal S}_P$
which are sparse, with non-zero values only on grid
elements surrounding the observation location.
Eq. (\ref{e.Lj}) implies that we can decompose $\LL$
into two sparse matrices as
\begin{equation}
\label{e.LL}
\LL = \Aa^T \Lambda^T ,
\end{equation}
with the non-zero elements in row $j$
of $\Aa$ just the coefficients $\alpha_{jk}$,
and columns of $\Lambda$ the field component
evaluation functionals for the full set of
observation locations.
$\Lambda$ depends only
on these observation locations (and possibly the
model parameter $\mm$, as we discuss below).  Observation
functionals (non-linear or linearized)
for any sort of EM data will be constructed
from the same field component functionals,
which are closely tied to the specific
numerical discretization scheme used.
$\Aa$ on the other hand, depends on details of
the observation functionals (e.g., impedance vs.
apparent resistivity), and
will also depend in general on the
background EM solution used for linearization, $\ee_0$.  However,
$\Aa$ does not depend on details of
the numerical discretization of the problem, e.g.
this matrix would take the same form for finite difference
and finite element numerical formulations.
We take advantage of this division in our
development of the modular system.

The field component evaluation functionals 
can depend on $\mm$ through the values of resistivity or
conductivity, defined on one of the conjugate grids
by the mapping we have denoted generically by
$\pi(\mm)$.  In this case we have
\begin{equation}
\label{e.QQ}
\QQ = \Aa^T \frac{\partial \Lambda(\pi(\mm))^T\ee_0}{\partial \mm}
= \Aa^T \left[ \left . \frac{\partial \Lambda(\pi))^T\ee_0}
{\partial \pi} \right|_{\pi(\mm_0)} \right]
\frac{\partial \pi(\mm)}{\partial \mm}
= \Aa^T \BB \frac{\partial \pi(\mm)}{\partial \mm}
= \tilde\QQ  \frac{\partial \pi(\mm)}{\partial \mm}.
\end{equation}
The matrices $\Aa$ and $\BB$ depend on the 
background model parameter $\mm_0$
through the EM solution $\ee_0$ and the grid
conductivity/resistivity $\pi(\mm_0)$, but they
do not depend on details of how the model is parameterized. 
Thus, as for $\PP$  (see (\ref{e.PP}))
the part of $\QQ$ which depends on details of the
model parameter implementation can generally be separated from those parts
which depend only on the specific data functionals 
($\Aa$) and only on the numerical grid ($\BB$).
The rows of the product $\tilde\QQ= \Aa^T\BB$ 
are elements of the space mapped onto by $\pi$, either
${\cal S}_P$ or ${\cal S}_A$ depending on the specific problem.

As specific examples, we consider implementation of
data functionals for 2D and 3D MT impedances.
Data functionals for other sorts of EM data (including
apparent resistivities and phases for MT)
can be derived in a similar way, with the linearized functionals
expressed as linear combinations of basic evaluation
functionals for electric and magnetic field components.
First consider the field component evaluation functionals
(columns of $\Lambda$).  For both of
these problems (and in many, but not all EM problems)
both primary and auxiliary field components are required
to compute predicted data from the primary 
EM solution $\ee$.  The auxiliary
fields (magnetics for our 3D example, and for the 2D TE mode,
electrics for the 2D TM mode), can be written in general
as $\tilde \ee = \TT \ee$, where $\TT:{\cal S}_P \mapsto {\cal S}_A$
is an appropriate discrete differential operator.  Then
\begin{equation}
\label{e.Lambda}
\Lambda = \left [ 
\begin{array}{c c}
\Lambda_P & \Lambda_A 
\end{array}
\right ] = 
\left [ 
\begin{array}{c c}
\Lambda_P & { \TT^T \tilde \Lambda_A }
\end{array}
\right ] ,
\end{equation}
where columns of $ \Lambda_P$ and $\tilde \Lambda_A$  are,
respectively, interpolation operators on the 
primary and auxiliary grids, respectively.
For the 3D MT example $\TT = (i\omega)^{-1}\CC$
maps from edges to faces, computing magnetic fields
through application of the discrete curl operator.
Interpolation
from edges and faces to an arbitrary location within
the 3D staggered grid model domain can be based on  something simple 
such as tri-linear splines.
In this case $\Lambda$ is independent of $\mm$, and $\QQ \equiv 0$.
Note that in some cases the evaluation functionals
can be somewhat more complicated than expression
(\ref{e.Lambda}) suggests.  
These details are not central to our general discussion,
but an example of such complications, which is relevant
to the TM MT problem, is discussed in the Appendix.

For the 2D TE mode case, $\TT = (-i\omega)^{-1}\OO\GG$, where $\OO$
is a diagonal operator with entries $+1$ and $-1$ for components
corresponding to $y$ and $z$ edges respectively.
Columns of 
$\Lambda_P$ and $\tilde \Lambda_A$ now represent bi-linear
spline interpolation from nodes and 2D edges,
respectively, to the data sites.
Again $\QQ \equiv 0$.  For 2D TM 
$\Lambda_P$ and $\tilde \Lambda_A$
are the same, but now
$\TT = {\bf diag} [ \rho (\mm) ] \OO \GG $, so
$\Lambda_A$ does depend on $\mm$, and $\QQ$ is non-zero.
From (\ref{e.QQ}) we have
\begin{equation}
\label{e.Qtilde}
\QQ = 
\Aa^T \left [
\begin{array}{c}
0 \\
\left( {\bf diag} 
[ \OO \GG \ee_0 ] \tilde \Lambda_A \right)^T
\end{array}
\right ]
\frac{\partial \rho}{\partial \mm}
\end{equation}
with the first two matrices in the product sparse and independent of
details of the model parameterization.

Finally, we derive the form of $\Aa$, and give more explicit
expressions for
$\LL$ and $\QQ$ for 2D and 3D MT impedances.
In the first case the data functional takes the form
$Z = \psi(\ee) =  \lambda_E^T\ee / \lambda_B^T\ee$.  From (\ref{e.Lj})
the row of $\LL$ corresponding to an impedance is
\begin{equation}
\label{e.LZ}
{\bf l}_j = {\bf l}_Z = (\lambda_B^T \ee_0)^{-1} \lambda_E - 
[ \lambda_E^T \ee_0 / (\lambda_B^T \ee_0 )^2] \lambda_B .
\end{equation}
In the TE case $\lambda_E$ and $\lambda_B = (-i\omega)^{-1}
\GG^T\OO^T \tilde \lambda_B$ are, respectively,
columns of $\Lambda_P$ and $\Lambda_A$, 
and $\lambda_E$ and $\tilde \lambda_B$
represent bi-linear spline interpolation
functionals on node (primary) and edge (auxiliary) spaces.
The coefficients $(\lambda_B^T \ee_0)^{-1}$ and
$[ \lambda_E^T \ee_0 / (\lambda_B^T \ee_0 )^2]$ are
the non-zero components of the $jth$ row of $\Aa$.
For the TM mode the role of primary and
auxiliary fields is reversed, 
with $\lambda_B$ representing interpolation of the
primary field and $\lambda_E = \GG^T\OO^T{\bf diag}[ \rho (\mm) ]
\tilde\lambda_E$.
The $jth$ row of $\QQ$ in the TM mode case is just
\begin{equation}
\label{e.QZ_TM}
{\bf q}_j = (\lambda_B \ee_0)^{-1} [ {\bf diag}[ \OO \GG \ee_0]
\tilde \lambda_E ]^T \frac{\partial\rho}{\partial\mm}.
\end{equation}
This is (a representation of)
a functional on the model parameter space, giving the
perturbation to the TM impedance due to a perturbation $\delta \mm$
of the model parameter.

In the 3D MT case the situation is somewhat more complicated.
Now the impedance is a $2 \times 2$ tensor,
the computation of which requires two independent solutions
computed for different source polarizations
\begin{equation}
\bf Z = {\cal E}{\cal B}^{-1} ,
\end{equation}
where $\cal E$ and $\cal B$
represent $2 \times 2$ matrices, consisting
of two components ($x$, $y$) of the electric or magnetic
fields evaluated for the two solutions
\begin{equation}
\label{e.Eij}
{\cal E}_{ij} = \lambda_{Ei} \ee_j
\,\,\,\,\,\,\,\,\,\,\,\,\,\,
{\cal B}_{ij} = \lambda_{Bi} \ee_j 
\,\,\,\,\,\,\,\,\,\,\,\,\,\,
i, \, j = 1,2 .
\end{equation}
Here index $i$ is used to distinguish between
the $x$ and $y$ components of the fields, 
and index $j$ distinguishes solutions for 
the two source polarizations.
$\lambda_{Ei}$ represents interpolation of component
$i$ defined on edges,
and $\lambda_{Bi} = (i\omega)^{-1}\tilde\lambda_{Bi}\CC$
where $\tilde\lambda_{Bi}$ represents interpolation
of component $i$ on faces.
The easiest way to derive the specific form for the linearization
(\ref{e.Lj}) of impedance component $Z_{ij}$ is to note
that perturbations to the computed
EM fields $\delta {\cal E}$ and $\delta {\cal B}$ 
result in the first order perturbation to the impedance tensor
\begin{equation}
\label{e.deltaZZ}
\delta \ZZ = \delta {\cal E} {\cal B}^{-1} - 
\ZZ^{-1} \delta {\cal B} {\cal B}^{-1} .
\end{equation}
Writing (\ref{e.deltaZZ}) out in components we find
for the perturbation to $Z_{ij}$
\begin{equation}
\label{e.ZZij}
\delta Z_{ij} = L_{Zij}(\delta \ee_1, \delta \ee_2) = 
\sum_{k=1}^{2} \left ( [{\cal B}^{-1}]_{kj} ( \lambda_{Ei}
-Z_{i1} \lambda_{B1} -Z_{i2} \lambda_{B2} ) \right ) \delta \ee_k .
\end{equation}

Thus the functionals for the
impedance (both non-linear and linearized) act
on the pair of solutions $\ee_1$, $\ee_2$,
and for the 3D MT problem this pair together must
be viewed as the fundamental EM solution object,
i.e., what is represented by $\ee$ in our abstract
treatment of sensitivity computations.
In terms of the two polarization components
the expression for the Jacobian
(\ref{e.SensModel}) really looks like
\begin{equation}
\label{e.JJ_3D}
\JJ = \left [
\begin{array}{c c}
\LL_1 & \LL_2 
\end{array}
\right]
\left [
\begin{array}{c c}
\Ss^{-1} & 0\\
0  &\Ss^{-1} 
\end{array}
\right ]
\left [
\begin{array}{c}
\PP_1 \\
\PP_2 
\end{array}
\right ]  = 
\left [ 
\begin{array}{c c}
\Aa_1^T  & \Aa_2^T 
\end{array}
\right]
\left [
\begin{array}{c c}
\Lambda & 0\\
0  & \Lambda 
\end{array}
\right ]
\left [
\begin{array}{c c}
\Ss^{-1} & 0\\
0  & \Ss^{-1} 
\end{array}
\right ]
\left [
\begin{array}{c }
\PP_1 \\
\PP_2 
\end{array}
\right ] .
\end{equation}
The solution operator ($\Ss^{-1}$), 
and the EM component evaluation functionals ($\Lambda$)
are the same for both modes,
but the operators $\Aa_k$ and $\PP_k$
are different.   Eq. (\ref{e.JJ_3D}) shows that multiplying
a model space object by $\JJ$, or
a data space object by $\JJ^T$,
requires two calls to the forward solver.

All of these complications
are hidden in the general expression 
(\ref{e.SensModel}), which remains valid provided we interpret
the component operators with enough generality.
This is an important constraint on a general
modular system: basic objects such as EM solutions
and data functionals should be constructed in such a way
that details such as the multiplicity of source polarizations
required for 3D MT can be hidden from
the general routines which implement 
Jacobian and gradient calculations.

At the same time a more careful examination of
(\ref{e.ZZij}) reveals that the impedance is degenerate in 
some sense, so that if the full Jacobian is to
be calculated some efficiencies are possible.
These efficiencies are potentially obscured in
an abstract treatment of the sensitivity calculation,
as they depend on specific features of a particular problem.
This is discussed in the Appendix, maybe.

\subsection{Multiple Transmitters}

Finally, some further comments are in order on 
the common {\it multiple transmitter} case,
e.g., with data from a range
of frequencies, 
or different geometrical transmitter configurations.
In both of these cases one has to solve separate forward
problems for each transmitter.
If $\JJ$, $\LL$, and $\QQ$ are partitioned consistent with
the data vector in (\ref{e.dPartition}),
the full sensitivity matrix can be expressed as 
\begin{equation}
\label{e.fullSens}
\JJ = \left(\begin{array}{c}
\JJ_1 \\ \vdots    \\ \JJ_{N_T}
\end{array} \right) =
\left(\begin{array}{c}
\LL_1{\bf S}_{1,\mm}^{-1} \PP_1 + \QQ_1 \\ \vdots
 \\ \LL_{N_T} {\bf S}_{{N_T},\mm}^{-1} \PP_{N_T} + \QQ_{N_T}
\end{array} \right) .
\end{equation}
The matrices
$\PP_l$ and $\QQ_l$ generally depend on the solution
for transmitter $l$.  The differential operator
for  the PDE ${\bf S}_{l,\mm}$ may also depend on
the transmitter, as, for example, in the MT cases
where the operator depends on frequency,
or be independent of $l$, if only the source geometry is modified.

In all of our detailed discussions of sensitivity
calculations we have focused on the operators for a single transmitter
$\JJ_l, \PP_l, \QQ_l$ and ${\bf S}_{l,\mm}$,
while in our discussions of inversion algorithms
$\JJ$ refers implicitly to the full multi-transmitter Jacobian.
However, multiplication by $\JJ$ or $\JJ^T$ involves
only simple concatenations or sums over the single
transmitter components, so all of the 
mathematical development in this section
remains relevant to the full problem.
Allowing for multiple transmitters, is however,
an important consideration in design of the
modular system, as we shall discuss in subsequent sections.
